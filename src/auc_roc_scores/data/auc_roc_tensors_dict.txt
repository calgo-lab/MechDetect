Nicholas Chandler
30.07.2025
Documentation/Datasheet of the data cube

The datasets herein have no more than 100,000 rows and no more than 500 columns

This datacube is a compressed dictionary mapping from OpenML dataset ids to 6D NumPy tensors.

Note that the dataset ids are not included in this dattasheet, you can find them by reading in the .npz file and printing the list of keys.

The dimensions of each tensor varies from cube to cube but follows the form:
(error_rate, error_mechanism, column, clean/dirty, case, cv_iter)

A few dimensions of this tensor are fixed values in the following order:
  - error_rate (5): 0.1, 0.25, 0.5, 0.75, 0.9
  - error_mechanism (3): MCAR, NAR, MNAR
  - clean/dirty (2): Clean, Dirty
  - case (4): Complete, Shuffled, Missing, Only Column j
  - cv_iter (10)
Yielding: (5, 3, n_cols, 2, 4, 10) -- thereby, only n_cols is dataset dependent

The values in the tensor are 10-fold cross validated roc_auc values for the various random forest models trained on predicting the error mask of the ith column given varying information.
The varying levels of information are described by the 4 scenarios (cases):
  1. A model is trained on all data to predict the error mask of column i.
  2. A model is trained on all data to predict a shuffled error mask of column i.
  3. A model is trained on all data except column i to predict the error mask of column i.
  4. A model is trained on only column i to predict the error mask of column i.

Good Luck!
